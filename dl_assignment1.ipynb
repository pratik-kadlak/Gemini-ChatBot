{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyThlU5Q/cBmGHmbcV5Gyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratik-kadlak/Gemini-ChatBot/blob/main/dl_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mAuPPqavEDm",
        "outputId": "6a1e737a-eb00-4139-abbc-a7e9c6717f19"
      },
      "execution_count": 188,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.41.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKB03cifU3KF"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import wandb\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)"
      ],
      "metadata": {
        "id": "Ow4bctElVK5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "KmodfLuwYKN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "lTD0krw6GwIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample_image_of_each_class():\n",
        "    \"\"\"\n",
        "        plots 1 image of each class in the training data\n",
        "    \"\"\"\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"DL_Assignment_1\"\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    images = []\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        if class_names[y_train[i]] not in labels:\n",
        "            labels.append(class_names[y_train[i]])\n",
        "            images.append(np.array(X_train[i]))\n",
        "        if(len(labels) == k):\n",
        "            break\n",
        "\n",
        "    wandb.log({\"Sample image for each class \": [wandb.Image(img, caption=caption) for img, caption in zip(images, labels)]})\n",
        "    wandb.finish()\n",
        "\n",
        "    # num_rows = 2\n",
        "    # num_cols = 5\n",
        "\n",
        "    # fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5))\n",
        "\n",
        "    # for i in range(len(images)):\n",
        "\n",
        "    #     #plotting 1 image from each class in wandb\n",
        "    #     wandb.log({\"Sample image for each class \": [wandb.Image(img, caption=caption) for img, caption in zip(images, labels)]})\n",
        "\n",
        "    #     row_idx = i // num_cols\n",
        "    #     col_idx = i % num_cols\n",
        "\n",
        "    #     axes[row_idx, col_idx].axis(\"off\")\n",
        "    #     axes[row_idx, col_idx].imshow(images[i], cmap=\"gray\")\n",
        "    #     axes[row_idx, col_idx].set_title(labels[i])\n",
        "\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "plot_sample_image_of_each_class()"
      ],
      "metadata": {
        "id": "vp-lMeA3VQ-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 2 & Question 3"
      ],
      "metadata": {
        "id": "wMMAx9CEVoXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "k = len(class_names)\n",
        "\n",
        "# loading the data\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#flattening the images, originally images is of size 28x28, converting it to 784x1\n",
        "X_train  = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
        "X_test = np.array(X_test)/255.0\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "1fY0Zd07z_3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_Wandb(neurons_per_layer, method):\n",
        "    \"\"\"\n",
        "        initializes weights and bias by the given method of initialization\n",
        "    \"\"\"\n",
        "\n",
        "    W = []\n",
        "    b = []\n",
        "    np.random.seed(42)\n",
        "\n",
        "    for l in range(len(neurons_per_layer)-1):\n",
        "        if method == \"random_uniform\":\n",
        "            W.append(np.random.uniform(-0.7, 0.7, (neurons_per_layer[l+1], neurons_per_layer[l])))\n",
        "            b.append(np.random.uniform(-0.7, 0.7, (neurons_per_layer[l+1],1)))\n",
        "        elif method == \"xavier\":\n",
        "            W.append(np.random.randn(neurons_per_layer[l+1],neurons_per_layer[l])*np.sqrt(6/(neurons_per_layer[l+1]+neurons_per_layer[l])))\n",
        "            b.append(np.zeros((neurons_per_layer[l+1], 1)))\n",
        "        else:\n",
        "            W.append(np.random.randn(neurons_per_layer[l+1], neurons_per_layer[l]) * 0.001)\n",
        "            b.append((np.random.randn(neurons_per_layer[l+1],1)) * 0.001)\n",
        "\n",
        "    return W, b"
      ],
      "metadata": {
        "id": "b0uHSicL1UN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation func and their derivatives\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.+np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2"
      ],
      "metadata": {
        "id": "MYtjJqjzMGax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions\n",
        "\n",
        "def cross_entropy(y, y_hat, W, weight_decay):\n",
        "    loss = 0\n",
        "    for i in range(len(y)):\n",
        "        for j in range(len(y[i])):\n",
        "            loss += -1.0 * y[i][j] * np.log(y_hat[i][j])\n",
        "\n",
        "    reg = 0\n",
        "    for i in range(len(W)):\n",
        "        reg += np.sum(np.square(W[i]))\n",
        "\n",
        "    regularized_loss = loss + weight_decay * reg\n",
        "    return regularized_loss\n",
        "\n",
        "\n",
        "def mean_square_error(y, y_hat, W, weight_decay):\n",
        "    loss = 0.5 * np.sum(np.square(y-y_hat))\n",
        "    reg = 0\n",
        "    for i in range(len(W)):\n",
        "        reg += np.sum(np.square(W[i]))\n",
        "\n",
        "    regularized_loss = loss + weight_decay * reg\n",
        "    return regularized_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "GQrWnf9CMOgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output functions\n",
        "\n",
        "def softmax(a):\n",
        "    return np.exp(a)/np.sum(np.exp(a), axis=0)\n",
        "\n",
        "\n",
        "# for back prop if loss func is mse\n",
        "def softmax_derivative(a):\n",
        "    return softmax(a)*(1-softmax(a))"
      ],
      "metadata": {
        "id": "86F1Id8eM8Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(W, b, X, y, num_hidden_layers, activation_func, weight_decay, loss_func):\n",
        "    \"\"\"\n",
        "        calculates loss and accuracy of the model\n",
        "    \"\"\"\n",
        "    y_hat, activation, preactivation = forward_propogation(W, b, X, num_hidden_layers, activation_func)\n",
        "    y_pred = []\n",
        "    for i in range(len(y_hat[0])):\n",
        "        y_pred.append(np.argmax(y_hat[:,i]))\n",
        "\n",
        "    acc = 0\n",
        "    for i in range(len(y)):\n",
        "        if y[i] == y_pred[i]:\n",
        "            acc += 1\n",
        "\n",
        "    acc = (acc * 100) / len(y)\n",
        "\n",
        "    y_one_hot = generate_one_hot_matrix(len(y), y)\n",
        "\n",
        "    if loss_func == \"cross_entropy\":\n",
        "        loss = cross_entropy(y_one_hot, y_hat, W, weight_decay)\n",
        "    else:\n",
        "        loss = cross_entropy(y_one_hot, y_hat, W, weight_decay)\n",
        "\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "vQj2UPMPbsQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propogation(W, b, X, num_hidden_layers, activation_func):\n",
        "    \"\"\"\n",
        "        does one forward pass of the data with the current weights and biases\n",
        "    \"\"\"\n",
        "\n",
        "    preactivation = []\n",
        "    activation = []\n",
        "\n",
        "    preactivation.append(X.T)\n",
        "    if activation_func == \"sigmoid\": activation.append(sigmoid(X.T))\n",
        "    elif activation_func == \"relu\": activation.append(relu(X.T))\n",
        "    else: activation.append(tanh(X.T))\n",
        "\n",
        "\n",
        "    for i in range(1, num_hidden_layers+1):\n",
        "        preactivation.append(np.matmul(W[i-1], activation[(i-1)]) + b[i-1])\n",
        "        if activation_func == \"sigmoid\":\n",
        "            activation.append(sigmoid(preactivation[i]))\n",
        "        elif activation_func == \"relu\":\n",
        "            activation.append(relu(preactivation[i]))\n",
        "        elif activation_func == \"tanh\":\n",
        "            activation.append(tanh(preactivation[i]))\n",
        "\n",
        "    preactivation.append(np.dot(W[-1], activation[-1]) + b[-1])\n",
        "    activation.append(softmax(preactivation[-1]))\n",
        "    y_hat = activation[-1]\n",
        "    return y_hat, activation, preactivation"
      ],
      "metadata": {
        "id": "3M1SND281WuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_one_hot_matrix(batch_size, y):\n",
        "    '''\n",
        "        generates one hot matrix, where the ith col gives the one hot vector for the ith image\n",
        "        and in that vector only the row number of true class will be 1 and rest will be zero\n",
        "    '''\n",
        "    y_one_hot = np.zeros((10,batch_size))\n",
        "    for i in range(batch_size):\n",
        "        y_one_hot[y[i]][i] = 1\n",
        "    return y_one_hot"
      ],
      "metadata": {
        "id": "dWcLm9xDfl7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update rules for various types of gradient descent\n",
        "\n",
        "def update_parameters(W, grad_W, b, grad_b, eta):\n",
        "    \"\"\"\n",
        "    normal gradient descent\n",
        "    \"\"\"\n",
        "    for i in range(0, len(W)):\n",
        "        W[i] = W[i] - eta * grad_W[i]\n",
        "        b[i] = b[i] - eta * grad_b[i]\n",
        "\n",
        "    return W, b\n",
        "\n",
        "\n",
        "def update_parameters_mgd(W, grad_W, b, grad_b, eta, beta, W_history, b_history):\n",
        "    \"\"\"\n",
        "    momentum based gradient descent\n",
        "    \"\"\"\n",
        "    for t in range(len(W)):\n",
        "        W_history[t] = beta * W_history[t] + grad_W[t]\n",
        "        b_history[t] = beta * b_history[t] + grad_b[t]\n",
        "    for i in range(len(W)):\n",
        "        W[i] = W[i] - eta * W_history[i]\n",
        "        b[i] = b[i] - eta * b_history[i]\n",
        "\n",
        "    return W, b, W_history, b_history\n",
        "\n",
        "\n",
        "def update_parameters_nag(W, W_history, b, b_history, eta, beta):\n",
        "    \"\"\"\n",
        "        nesterov accelerated gradient descent\n",
        "    \"\"\"\n",
        "    for i in range(len(W)):\n",
        "        W_history[i] = beta * W_history[i]\n",
        "        b_history[i] = beta * b_history[i]\n",
        "\n",
        "    for i in range(len(W)):\n",
        "        W[i] = W[i] - eta * W_history[i]\n",
        "        b[i] = b[i] - eta * b_history[i]\n",
        "\n",
        "    return W, b, W_history, b_history\n",
        "\n",
        "\n",
        "def update_parameters_rmsprop(W, grad_W, b, grad_b, vt_W, vt_b, eta, beta, epsilon):\n",
        "    \"\"\"\n",
        "        rmsprop gradient descent\n",
        "    \"\"\"\n",
        "    for i in range(len(grad_W)):\n",
        "        vt_W[i] = beta * vt_W[i] + (1 - beta) * np.square(grad_W[i])\n",
        "        vt_b[i] = beta * vt_b[i] + (1 - beta) * np.square(grad_b[i])\n",
        "\n",
        "    for i in range(len(W)):\n",
        "        W[i] = W[i] - (eta/np.sqrt(vt_W[i]+epsilon)) * grad_W[i]\n",
        "        b[i] = b[i] - (eta/np.sqrt(vt_b[i]+epsilon)) * grad_b[i]\n",
        "\n",
        "    return W, b, vt_W, vt_b\n",
        "\n",
        "\n",
        "def update_parameters_adam(W, grad_W, vt_W, mt_W, b, grad_b, vt_b, mt_b, t, eta, beta1, beta2, epsilon):\n",
        "    for i in range(len(W)):\n",
        "        curr_mt_W = beta1 * mt_W[i] + (1 - beta1) * grad_W[i]\n",
        "        curr_mt_b = beta1 * mt_b[i] + (1 - beta1) * grad_b[i]\n",
        "\n",
        "        curr_vt_W = beta2 * vt_W[i] + (1 - beta2) * np.square(grad_W[i])\n",
        "        curr_vt_b = beta2 * vt_b[i] + (1 - beta2) * np.square(grad_b[i])\n",
        "\n",
        "        mt_W_hat = curr_mt_W / (1.0 - beta1**t)\n",
        "        mt_b_hat = curr_mt_b / (1.0 - beta1**t)\n",
        "\n",
        "        vt_W_hat = curr_vt_W / (1.0 - beta2**t)\n",
        "        vt_b_hat = curr_vt_b / (1.0 - beta2**t)\n",
        "\n",
        "        # saving for the next iteration\n",
        "        mt_W[i] = curr_mt_W\n",
        "        mt_b[i] = curr_mt_b\n",
        "        vt_W[i] = curr_vt_W\n",
        "        vt_b[i] = curr_vt_b\n",
        "\n",
        "        # updating the parameters\n",
        "        W[i] = W[i] - (eta/(np.sqrt(vt_W_hat) + epsilon)) * mt_W_hat\n",
        "        b[i] = b[i] - (eta/(np.sqrt(vt_b_hat) + epsilon)) * mt_b_hat\n",
        "\n",
        "    return W, b, vt_W, vt_b, mt_W, mt_b\n",
        "\n",
        "\n",
        "def update_parameters_nadam(W, grad_W, vt_W, mt_W, b, grad_b, vt_b, mt_b, t, eta, beta1, beta2, epsilon):\n",
        "    for i in range(len(W)):\n",
        "        curr_mt_W = beta1 * mt_W[i] + (1 - beta1) * grad_W[i]\n",
        "        curr_mt_b = beta1 * mt_b[i] + (1 - beta1) * grad_b[i]\n",
        "\n",
        "        curr_vt_W = beta2 * vt_W[i] + (1 - beta2) * np.square(grad_W[i])\n",
        "        curr_vt_b = beta2 * vt_b[i] + (1 - beta2) * np.square(grad_b[i])\n",
        "\n",
        "        mt_W_hat = curr_mt_W / (1.0 - beta1**t)\n",
        "        mt_b_hat = curr_mt_b / (1.0 - beta1**t)\n",
        "\n",
        "        vt_W_hat = curr_vt_W / (1.0 - beta2**t)\n",
        "        vt_b_hat = curr_vt_b / (1.0 - beta2**t)\n",
        "\n",
        "        # saving for the next iteration\n",
        "        mt_W[i] = curr_mt_W\n",
        "        mt_b[i] = curr_mt_b\n",
        "        vt_W[i] = curr_vt_W\n",
        "        vt_b[i] = curr_vt_b\n",
        "\n",
        "        # updating the parameters\n",
        "        W[i] = W[i] - (eta/(np.sqrt(vt_W_hat) + epsilon)) * ((1-beta1)*grad_W[i]/(1-beta1**t))\n",
        "        b[i] = b[i] - (eta/(np.sqrt(vt_b_hat) + epsilon)) * ((1-beta1)*grad_b[i]/(1-beta1**t))\n",
        "\n",
        "    return W, b, vt_W, vt_b, mt_W, mt_b"
      ],
      "metadata": {
        "id": "I4n8rlFsWI2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propogation(W, b, y_one_hot, activation, preactivation, L, activation_func, loss_func):\n",
        "    \"\"\"\n",
        "        back_propogation algorithm for updating the parameters\n",
        "    \"\"\"\n",
        "\n",
        "    grad_preactivation = []\n",
        "\n",
        "    # grad with respect to output units\n",
        "    if loss_func == 'cross_entropy':\n",
        "        grad_preactivation.append(activation[L]-y_one_hot)\n",
        "    else:\n",
        "        grad_preactivation.append(activation[L]-y_one_hot)\n",
        "\n",
        "    grad_W = []\n",
        "    grad_b = []\n",
        "\n",
        "    for i in range(L, 0, -1):\n",
        "        # grad with respect to weights and biases\n",
        "        grad_W.append(np.matmul(grad_preactivation[-1], activation[i-1].T))\n",
        "        grad_b.append(np.sum(grad_preactivation[-1], axis=1, keepdims=True))\n",
        "\n",
        "        if i == 1:\n",
        "            break\n",
        "\n",
        "        # grad with respect to hidden units\n",
        "        grad_hi = np.matmul(W[i-1].T, grad_preactivation[-1])\n",
        "\n",
        "        if activation_func == \"sigmoid\":\n",
        "            grad_preactivation.append(np.multiply(grad_hi, sigmoid_derivative(preactivation[i-1])))\n",
        "        elif activation_func == \"relu\":\n",
        "            grad_preactivation.append(np.multiply(grad_hi, relu_derivative(preactivation[i-1])))\n",
        "        elif activation_func == \"tanh\":\n",
        "            grad_preactivation.append(np.multiply(grad_hi, tanh_derivative(preactivation[i-1])))\n",
        "\n",
        "    return grad_W[::-1], grad_b[::-1]"
      ],
      "metadata": {
        "id": "9PM_XFiuI5Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_val_data(X, y, validation_percent=0.1, random_seed=None):\n",
        "    \"\"\"\n",
        "    Split the data into training and validation sets.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    num_samples = len(X)\n",
        "    num_val_samples = int(validation_percent * num_samples)\n",
        "\n",
        "    # Randomly shuffle indices\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Split data based on shuffled indices\n",
        "    val_indices = indices[:num_val_samples]\n",
        "    train_indices = indices[num_val_samples:]\n",
        "\n",
        "    X_train, X_val = X[train_indices], X[val_indices]\n",
        "    y_train, y_val = y[train_indices], y[val_indices]\n",
        "\n",
        "    return X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "sHk7d1RJElEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X_train, X_val, y_train, y_val, config):\n",
        "    # # setting hyper parameters\n",
        "    num_hidden_layers = config.num_hidden_layers\n",
        "    size_of_hidden_layer = config.size_of_hidden_layer\n",
        "    activation_func = config.activation_func\n",
        "    loss_func = \"cross_entropy\"\n",
        "    method = config.method\n",
        "    num_images = len(X_train)\n",
        "    batch_size = config.batch_size\n",
        "    epoch = config.epochs\n",
        "    eta = config.eta\n",
        "    weight_decay = config.weight_decay\n",
        "    optimizer = config.optimizer\n",
        "\n",
        "    # used for momentum\n",
        "    beta = 0.5\n",
        "    W_history = [0] * (num_hidden_layers+1)\n",
        "    b_history = [0] * (num_hidden_layers+1)\n",
        "\n",
        "    # used for rmsprop\n",
        "    vt_W = [0] * (num_hidden_layers+1)\n",
        "    vt_b = [0] * (num_hidden_layers+1)\n",
        "    epsilon = 1e-8\n",
        "    beta = 0.5\n",
        "\n",
        "    # usef for adam and nadam\n",
        "    vt_W = [0] * (num_hidden_layers+1)\n",
        "    vt_b = [0] * (num_hidden_layers+1)\n",
        "    mt_W = [0] * (num_hidden_layers+1)\n",
        "    mt_b = [0] * (num_hidden_layers+1)\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    epsilon = 1e-8\n",
        "    t = 1\n",
        "\n",
        "    # usef for nadam\n",
        "\n",
        "    # making the list of structure of neural networks\n",
        "    neurons_per_layer = [X_train.shape[1]]\n",
        "    for i in range(num_hidden_layers):\n",
        "        neurons_per_layer.append(size_of_hidden_layer)\n",
        "    neurons_per_layer.append(k)\n",
        "\n",
        "\n",
        "    # Initialize W, b\n",
        "    W, b = initialize_Wandb(neurons_per_layer, method)\n",
        "    # y_one_hot = generate_one_hot(num_images, y_train)\n",
        "\n",
        "    y_hat = []\n",
        "    for iteration in tqdm(range(epoch)):\n",
        "        for i in range(0, num_images, batch_size):\n",
        "            if i + batch_size > num_images:\n",
        "                X_batch = X_train[i:]\n",
        "                y_batch = y_train[i:]\n",
        "                batch_size = len(X_batch)\n",
        "            else:\n",
        "                X_batch = X_train[i:i+batch_size]\n",
        "                y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "            if optimizer == \"momentum\":\n",
        "                hL, activation, preactivation = forward_propogation(W, b, X_batch, num_hidden_layers, activation_func)\n",
        "                y_one_hot = generate_one_hot_matrix(batch_size, y_batch)\n",
        "                grad_W, grad_b = backward_propogation(W, b, y_one_hot, activation, preactivation, num_hidden_layers+1, activation_func, \"cross_entropy\")\n",
        "                W, b, W_history, b_history = update_parameters_mgd(W, grad_W, b, grad_b, eta, beta, W_history, b_history)\n",
        "            elif optimizer == \"nag\":\n",
        "                W_look_ahead, b_look_ahead, W_history, b_history = update_parameters_nag(W, W_history, b, b_history, eta, beta) # updating by history\n",
        "                hL, activation, preactivation = forward_propogation(W_look_ahead, b_look_ahead, X_batch, num_hidden_layers, activation_func)\n",
        "                y_one_hot = generate_one_hot_matrix(batch_size, y_batch)\n",
        "                grad_W, grad_b = backward_propogation(W, b, y_one_hot, activation, preactivation, num_hidden_layers+1, activation_func, loss_func)\n",
        "                W, b = update_parameters(W, grad_W, b, grad_b, eta) # updating by the grad of lookahead point\n",
        "            elif optimizer == \"rmsprop\":\n",
        "                hL, activation, preactivation = forward_propogation(W, b, X_batch, num_hidden_layers, activation_func)\n",
        "                y_one_hot = generate_one_hot_matrix(batch_size, y_batch)\n",
        "                grad_W, grad_b = backward_propogation(W, b, y_one_hot, activation, preactivation, num_hidden_layers+1, activation_func, loss_func)\n",
        "                W, b, vt_W, vt_b = update_parameters_rmsprop(W, grad_W, b, grad_b, vt_W, vt_b, eta, beta, epsilon)\n",
        "            elif optimizer == \"adam\":\n",
        "                hL, activation, preactivation = forward_propogation(W, b, X_batch, num_hidden_layers, activation_func)\n",
        "                y_one_hot = generate_one_hot_matrix(batch_size, y_batch)\n",
        "                grad_W, grad_b = backward_propogation(W, b, y_one_hot, activation, preactivation, num_hidden_layers+1, activation_func, loss_func)\n",
        "                W, b, vt_W, vt_b, mt_W, mt_b = update_parameters_adam(W, grad_W, vt_W, mt_W, b, grad_b, vt_b, mt_b, t, eta, beta1, beta2, epsilon)\n",
        "                t += 1\n",
        "            elif optimizer == \"nadam\":\n",
        "                W_look_ahead, b_look_ahead, vt_W, vt_b = update_parameters_nag(W, W_history, b, b_history, eta, beta) # updating by history\n",
        "                hL, activation, preactivation = forward_propogation(W_look_ahead, b_look_ahead, X_batch, num_hidden_layers, activation_func)\n",
        "                y_one_hot = generate_one_hot_matrix(batch_size, y_batch)\n",
        "                grad_W, grad_b = backward_propogation(W, b, y_one_hot, activation, preactivation, num_hidden_layers+1, activation_func, loss_func)\n",
        "                W, b, vt_W, vt_b, mt_W, mt_b = update_parameters_nadam(W, grad_W, vt_W, mt_W, b, grad_b, vt_b, mt_b, t, eta, beta1, beta2, epsilon)\n",
        "                t += 1\n",
        "            else:\n",
        "                hL, activation, preactivation = forward_propogation(W, b, X_batch, num_hidden_layers, activation_func)\n",
        "                y_one_hot = generate_one_hot_matrix(batch_size, y_batch)\n",
        "                grad_W, grad_b = backward_propogation(W, b, y_one_hot, activation, preactivation, num_hidden_layers+1, activation_func, loss_func)\n",
        "                W, b = update_parameters(W, grad_W, b, grad_b, eta)\n",
        "\n",
        "\n",
        "        val_accuracy, val_loss = evaluate_model(W, b, X_val, y_val, num_hidden_layers, activation_func, weight_decay, loss_func)\n",
        "        train_accuracy, train_loss = evaluate_model(W, b, X_train, y_train, num_hidden_layers, activation_func, weight_decay, loss_func)\n",
        "        print(f\" val_accuracy:{val_accuracy}, val_loss:{val_loss}, train_accuracy:{train_accuracy}, train_loss:{train_loss}\")\n",
        "        wandb.log({\"val_accuracy\":val_accuracy, 'val_loss':val_loss, \"train_accuracy\":train_accuracy, \"train_loss\":train_loss, \"epoch\":iteration+1})\n",
        "\n",
        "    hL, activation, preactivation = forward_propogation(W, b, X_test, num_hidden_layers, activation_func)\n",
        "    y_pred = []\n",
        "    for i in range(len(hL[0])):\n",
        "        y_pred.append(hL[:, i])\n",
        "\n",
        "    plot_confusion_matrix(list(y_test), y_pred)\n"
      ],
      "metadata": {
        "id": "Ng__TfcbFmBq"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7"
      ],
      "metadata": {
        "id": "uJgyt1YHRBSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_pred, y_test):\n",
        "    wandb.log({\"confusion_matrix\" : wandb.plot.confusion_matrix(y_true=y_test, preds=y_pred, class_names=None)})"
      ],
      "metadata": {
        "id": "_ESRLE4Kqz50"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_test_accuracy(X_test, y_test, W, b, num_hidden_layers, activation_func):\n",
        "    hL, activation, preactivation = forward_propogation(W, b, X_test, num_hidden_layers, activation_func)\n",
        "    y_pred = []\n",
        "    for i in range(len(hL[0])):\n",
        "        y_pred.append(np.argmax(hL[:, i]))\n",
        "\n",
        "    count = 0\n",
        "    for i in range(len(y_test)):\n",
        "        if y_test[i] == y_pred[i]:\n",
        "            count += 1\n",
        "\n",
        "    return count * 100 / len(y_test)"
      ],
      "metadata": {
        "id": "P440bBynrMhb"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = split_train_val_data(X_train, y_train)\n",
        "\n",
        "# wandb.init(\"DL_Assignment_1\")\n",
        "\n",
        "sweep_config = {\n",
        "\"name\": \"Cross Entropy Loss\",\n",
        "\"metric\": {\n",
        "    \"name\":\"val_accuracy\",\n",
        "    \"goal\": \"maximize\"\n",
        "},\n",
        "\"method\": \"bayes\",\n",
        "\"parameters\": {\n",
        "        \"eta\": {\n",
        "            \"values\": [1e-3, 1e-4]\n",
        "        },\n",
        "        \"activation_func\": {\n",
        "            \"values\": [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "        },\n",
        "        \"method\": {\n",
        "            \"values\": [\"xavier\", \"random_uniform\", \"random_normal\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            # \"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "            \"values\": [\"nadam\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16,32]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.0005, 0.05]\n",
        "        },\n",
        "        \"size_of_hidden_layer\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \"num_hidden_layers\": {\n",
        "            \"values\": [3, 4, 5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "        gradient_descent(X_train, X_val, y_train, y_val, config)\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"DL_Assignment_1\")\n",
        "wandb.agent(sweep_id, train, count = 1)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrQ-vcw-txL6",
        "outputId": "fe5b4f49-95a5-4382-c8f7-b1f9bba68d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: jvg7scln\n",
            "Sweep URL: https://wandb.ai/space_monkeys/DL_Assignment_1/sweeps/jvg7scln\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MP_WtHz9wG1C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}